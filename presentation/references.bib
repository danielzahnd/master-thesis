% This file was created with Citavi 6.4.0.35

% This file was created with Citavi 6.4.0.35

@book{Daily.2018,
 author = {Daily, John W.},
 year = {2018},
 title = {Statistical Thermodynamics},
 publisher = {{Cambridge University Press}},
 isbn = {9781108233194},
 doi = {10.1017/9781108233194}
}


@article{Froehlich.2004,
	author = {Froehlich, Claus and Lean, Judith},
	year = {2004},
	title = {Solar radiative output and its variability: evidence and mechanisms},
	pages = {273--320},
	volume = {12},
	number = {4},
	issn = {0935-4956},
	journal = {The Astronomy and Astrophysics Review},
	doi = {10.1007/s00159-004-0024-1}
}

@book{Liou.2002,
 abstract = {This Second Edition of An Introduction to Atmospheric Radiation has been extensively revised to address the fundamental study and quantitative measurement of the interactions of solar and terrestrial radiation with molecules, aerosols, and cloud particles in planetary atmospheres. It contains 70{\%} new material, much of it stemming from the investigation of the atmospheric greenhouse effects of external radiative perturbations in climate systems, and the development of methodologies for inferring atmospheric and surface parameters by means of remote sensing. Liou's comprehensive treatment of the},
 author = {Liou, Kuo-Nan},
 year = {2002},
 title = {An introduction to atmospheric radiation},
 url = {https://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=195011},
 address = {Amsterdam and Boston},
 edition = {2nd ed.},
 volume = {v. 84},
 publisher = {{Academic Press}},
 isbn = {0-12-451451-0},
 series = {International geophysics series}
}




@article{weng2018flow,
  title   = "Flow-based Deep Generative Models",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2018",
  url     = "https://lilianweng.github.io/posts/2018-10-13-flow-models/"
}

% This file was created with Citavi 6.4.0.35

@article{LaCruzRodriguez.2019,
 abstract = {Understanding the complex dynamics and structure of the upper solar atmosphere strongly benefits from the use of a combination of several diagnostics. Frequently, such diverse diagnostics can only be obtained from telescopes and/or instrumentation operating at widely different spatial resolution. To optimize the utilization of such data, we propose a new method for the global inversion of data acquired at different spatial resolution. The method has its roots in the Levenberg-Marquardt algorithm but involves the use of linear operators to transform and degrade the synthetic spectra of a highly resolved guess model to account for the effects of spatial resolution, data sampling, alignment, and image rotation of each of the datasets. We have carried out a list of numerical experiments to show that our method allows for the extraction of spatial information from two simulated datasets that have gone through two different telescope apertures and that are sampled in different spatial grids. Our results show that each dataset contributes in the inversion by constraining information at the spatial scales that are present in each of the datasets, and no negative effects are derived from the combination of multiple resolution data. This method is especially relevant for chromospheric studies that attempt to combine datasets acquired with different telescopes and/or datasets acquired at different wavelengths. The techniques described in the present study will also help to address the ever increasing resolution gap between space-borne missions and forthcoming ground-based facilities.},
 author = {de {La Cruz Rodr{\'i}guez}, J.},
 year = {2019},
 title = {A method for global inversion of multi-resolution solar data},
 pages = {A153},
 volume = {631},
 issn = {0004-6361},
 journal = {Astronomy {\&} Astrophysics},
 doi = {10.1051/0004-6361/201936635},
 file = {La Cruz Rodríguez 2019 - A method for global inversion:C\:\\Users\\zahnd\\Documents\\Citavi 6\\Projects\\Master thesis references\\Citavi Attachments\\La Cruz Rodríguez 2019 - A method for global inversion.pdf:pdf}
}




@software{nflows,
  author       = {Conor Durkan and
                  Artur Bekasov and
                  Iain Murray and
                  George Papamakarios},
  title        = {{nflows}: normalizing flows in {PyTorch}},
  month        = nov,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {v0.14},
  doi          = {10.5281/zenodo.4296287},
  url          = {https://doi.org/10.5281/zenodo.4296287}
}

@book{Weigert.2006,
 author = {Weigert, Alfred and Wendker, Heinrich J. and Wisotzki, Lutz},
 year = {2006},
 title = {Astronomie und Astrophysik: Ein Grundkurs},
 address = {Weinheim},
 edition = {1. Nachdr. der 4., v{\"o}llig {\"u}berarb. und erw. Aufl.},
 publisher = {{Wiley-VCH Verlag}},
 isbn = {3527403582},
 series = {Lehrbuch Physik}
}

@book{delToroIniesta.2003,
 abstract = {This book presents a complete overview of spectropolarimetry for graduates and researchers.



Cover -- Half-title -- Title -- Copyright -- Dedication -- Contents -- Preface -- Acknowledgements -- 1 Historical introduction -- 1.1 Early discoveries in polarization -- 1.2 A mathematical formulation of polarization -- 1.3 Discovery of the Zeeman effect -- 1.4 Radiative transfer for polarized light -- Recommended bibliography -- 2 A review of some basic concepts -- 2.1 Light as an electromagnetic wave -- 2.2 The monochromatic, time-harmonic plane wave -- 2.3 The polarization tensor -- 2.4 The Stokes parameters of a monochromatic, time-harmonic plane wave -- Recommended bibliography -- 3 The polarization properties of quasi-monochromatic light -- 3.1 Polychromatic light as a statistical superposition of monochromatic light -- 3.2 The quasi-monochromatic plane wave -- 3.3 The polarization tensor and the Stokes parameters of a quasi-monochromatic plane wave -- 3.4 Degree of polarization and the Poincar{\'e} sphere -- 3.5 Measuring the polarization state of quasi-monochromatic light -- 3.6 A further perspective on polarization properties -- Recommended bibliography -- 4 Linear optical systems acting on polarized light -- 4.1 Propagation of light through anisotropic media -- 4.1.1 Measurable effects of anisotropy -- 4.2 The extraordinary index of refraction and the energy propagation direction -- 4.3 Some notational conventions -- 4.4 Transforming the polarization state of light -- 4.5 The Mueller matrix and some of its properties -- 4.6 Block components of (solar) polarimeters -- 4.6.1 Rotation of the reference frame -- 4.6.2 The linear analyzer-polarizer -- 4.6.3 The partial linear polarizer -- 4.6.4 The linear retarder -- 4.6.5 The Mueller matrix of an optical train -- 4.7 Spatial and temporal modulation -- 4.7.1 Spatial modulation -- 4.7.1.1 The Babinet compensator -- 4.7.1.2 The double birefringent plate -- 4.7.1.3 Polarizing beam splitter.},
 author = {{del Toro Iniesta}, Jose Carlos},
 year = {2003},
 title = {Introduction to Spectropolarimetry},
 url = {https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=202396},
 address = {Cambridge},
 publisher = {{Cambridge University Press}},
 isbn = {0-521-81827-3}
}

@book{Griffiths.2018,
 abstract = {Changes and additions to the new edition of this classic textbook include: new chapter on Symmetries, new problems and examples, improved explanations, more numerical problems to be worked on a computer, new applications to solid state physics, consolidated treatment of time-dependent potentials.},
 author = {Griffiths, David J. and Schroeter, Darrell F.},
 year = {2018},
 title = {Introduction to quantum mechanics},
 address = {Cambridge},
 edition = {Third edition},
 publisher = {{Cambridge University Press}},
 isbn = {978-1-107-18963-8},
 doi = {10.1017/9781316995433}
}

@book{BellotRubio.2017,
 author = {{Bellot Rubio}, Luis R. and {Ruiz Cobo}, B.},
 year = {2017},
 title = {Inversion of Stokes profiles with SIR},
 institution = {{Instituto de Astrofisica de Andalucia} and {Instituto de Astrofisica de Canarias}}
}


@book{UitenbroekNSME.2020,
 author = {Uitenbroek, Han},
 year = {2020},
 title = {Polarized Radiative Transfer: Milne-Eddington Inversions Workshop - Lecture notes},
 institution = {{NSO}},
 publisher = {National Solar Observatory}
}

@book{UitenbroekSME.2020,
 author = {Uitenbroek, Han},
 year = {2020},
 title = {The Milne-Eddington Approximation: Milne-Eddington Inversions Workshop - Lecture notes},
 institution = {{NSO}},
 publisher = {National Solar Observatory}
}

@book{Amini.2023,
 author = {Amini, Alexander},
 year = {2023},
 title = {Introduction to Deep Learning - Lecture notes},
 institution = {{MIT}},
 publisher = {Massachusetts Institute of Technology}
}

@book{Rutten.2015,
 author = {Rutten, Robert J.},
 year = {2015},
 title = {Introduction to astrophysical radiative transfer: Lecture notes},
 institution = {{Sterrekundig Instituut Utrecht}}
}

@book{Rutten.2015,
 author = {Rutten, Robert J.},
 year = {2015},
 title = {Introduction to astrophysical radiative transfer: Lecture notes},
 institution = {{Sterrekundig Instituut Utrecht}}
}

@book{Rutten.2003,
 author = {Rutten, Robert J.},
 year = {2003},
 title = {Radiative transfer in stellar atmospheres: Lecture notes},
 institution = {{Sterrekundig Instituut Utrecht}}
}


@book{Rutten.1993,
 author = {Rutten, Robert J.},
 year = {1993},
 title = {Solar spectrum formation: Lecture notes},
 institution = {{Sterrekundig Instituut Utrecht}}
}


@article{RuizCobo.1992,
 author = {{Ruiz Cobo}, B. and {del Toro Iniesta}, J. C.},
 year = {1992},
 title = {Inversion of Stokes profiles},
 pages = {375},
 volume = {398},
 issn = {0004-637X},
 journal = {The Astrophysical Journal},
 doi = {10.1086/171862}
}


@misc{Rezende.21.05.2015,
 abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
 author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
 date = {21.05.2015},
 title = {Variational Inference with Normalizing Flows},
 year = {2015},
 url = {http://arxiv.org/pdf/1505.05770v6},
 file = {http://arxiv.org/abs/1505.05770v6},
 file = {https://arxiv.org/pdf/1505.05770v6.pdf}
}


@book{Raschka.2022,
 abstract = {bThis book of the bestselling and widely acclaimed Python Machine Learning series is a comprehensive guide to machine and deep learning using PyTorch's simple to code framework/bh4Key Features/h4ulliLearn applied machine learning with a solid foundation in theory/liliClear, intuitive explanations take you deep into the theory and practice of Python machine learning/liliFully updated and expanded to cover PyTorch, transformers, XGBoost, graph neural networks, and best practices/li/ulh4Book Description/h4Machine Learning with PyTorch and Scikit-Learn is a comprehensive guide to machine learning and deep learning with PyTorch. It acts as both a step-by-step tutorial and a reference you'll keep coming back to as you build your machine learning systems. Packed with clear explanations, visualizations, and examples, the book covers all the essential machine learning techniques in depth. While some books teach you only to follow instructions, with this machine learning book, we teach the principles allowing you to build models and applications for yourself. Why PyTorch? PyTorch is the Pythonic way to learn machine learning, making it easier to learn and simpler to code with. This book explains the essential parts of PyTorch and how to create models using popular libraries, such as PyTorch Lightning and PyTorch Geometric. You will also learn about generative adversarial networks (GANs) for generating new data and training intelligent agents with reinforcement learning. Finally, this new edition is expanded to cover the latest trends in deep learning, including graph neural networks and large-scale transformers used for natural language processing (NLP). This PyTorch book is your companion to machine learning with Python, whether you're a Python developer new to machine learning or want to deepen your knowledge of the latest developments.h4What you will learn/h4ulliExplore frameworks, models, and techniques for machines to 'learn' from data/liliUse scikit-learn for machine learning and PyTorch for deep learning/liliTrain machine learning classifiers on images, text, and more/liliBuild and train neural networks, transformers, and boosting algorithms/liliDiscover best practices for evaluating and tuning models/liliPredict continuous target outcomes using regression analysis/liliDig deeper into textual and social media data using sentiment analysis/li/ulh4Who this book is for/h4If you have a good grasp of Python basics and want to start learning about machine learning and deep learning, then this is the book for you. This is an essential resource written for developers and data scientists who want to create practical machine learning and deep learning applications using scikit-learn and PyTorch.Before you get started with this book, you'll need a good understanding of calculus, as well as linear algebra.},
 author = {Raschka, Sebastian},
 year = {2022},
 title = {Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python},
 url = {https://www.wiso-net.de/document/PKEB__9781801816380770},
 address = {Birmingham},
 edition = {1},
 publisher = {{Packt Publishing Limited}},
 isbn = {9781801819312}
}


@article{Papamakarios.,
 abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
 author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
 title = {Normalizing Flows for Probabilistic Modeling and Inference},
 url = {http://arxiv.org/pdf/1912.02762v2},
 journal = {Journal of Machine Learning Research},
 file = {http://arxiv.org/abs/1912.02762v2},
 file = {https://arxiv.org/pdf/1912.02762v2.pdf}
}


@book{Mitchell.1997,
 author = {Mitchell, Tom M.},
 year = {1997},
 title = {Machine learning},
 url = {http://www.loc.gov/catdir/description/mh022/97007692.html},
 keywords = {Computer algorithms;Machine learning},
 address = {New York, NY},
 edition = {International ed.},
 publisher = {McGraw-Hill},
 isbn = {0070428077},
 series = {McGraw-Hill international editions},
 file = {https://swbplus.bsz-bw.de/bsz079140394abs.htm},
 file = {https://zbmath.org/?q=an:0913.68167}
}


@misc{Lippe.17.06.2020,
 abstract = {Despite their popularity, to date, the application of normalizing flows on categorical data stays limited. The current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. Instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. In this paper, we investigate \emph{Categorical Normalizing Flows}, that is normalizing flows for categorical data. By casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. Using a factorized decoder, we introduce an inductive bias to model any interactions in the normalizing flow. As a consequence, we do not only simplify the optimization compared to having a joint decoder, but also make it possible to scale up to a large number of categories that is currently impossible with discrete normalizing flows. Based on Categorical Normalizing Flows, we propose GraphCNF a permutation-invariant generative model on graphs. GraphCNF implements a three step approach modeling the nodes, edges and adjacency matrix stepwise to increase efficiency. On molecule generation, GraphCNF outperforms both one-shot and autoregressive flow-based state-of-the-art.},
 author = {Lippe, Phillip and Gavves, Efstratios},
 date = {17.06.2020},
 title = {Categorical Normalizing Flows via Continuous Transformations},
 url = {http://arxiv.org/pdf/2006.09790v3},
 file = {https://arxiv.org/pdf/2006.09790v3.pdf},
 file = {http://arxiv.org/abs/2006.09790v3}
}


@article{Kobyzev.2021,
 abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
 author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
 year = {2021},
 title = {Normalizing Flows: An Introduction and Review of Current Methods},
 url = {http://arxiv.org/pdf/1908.09257v4},
 pages = {3964--3979},
 volume = {43},
 number = {11},
 issn = {0162-8828},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 doi = {10.1109/TPAMI.2020.2992934},
 file = {https://arxiv.org/pdf/1908.09257v4.pdf},
 file = {http://arxiv.org/abs/1908.09257v4}
}


@misc{Kingma.09.07.2018,
 abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow},
 author = {Kingma, Diederik P. and Dhariwal, Prafulla},
 date = {09.07.2018},
 title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
 url = {http://arxiv.org/pdf/1807.03039v2},
 file = {http://arxiv.org/abs/1807.03039v2},
 file = {https://arxiv.org/pdf/1807.03039v2.pdf}
}


@misc{Hoogeboom.30.01.2020,
 abstract = {Media is generally stored digitally and is therefore discrete. Many successful deep distribution models in deep learning learn a density, i.e., the distribution of a continuous random variable. Na\{\textquotedbl}ive optimization on discrete data leads to arbitrarily high likelihoods, and instead, it has become standard practice to add noise to datapoints. In this paper, we present a general framework for dequantization that captures existing methods as a special case. We derive two new dequantization objectives: importance-weighted (iw) dequantization and R\'enyi dequantization. In addition, we introduce autoregressive dequantization (ARD) for more flexible dequantization distributions. Empirically we find that iw and R\'enyi dequantization considerably improve performance for uniform dequantization distributions. ARD achieves a negative log-likelihood of 3.06 bits per dimension on CIFAR10, which to the best of our knowledge is state-of-the-art among distribution models that do not require autoregressive inverses for sampling.},
 author = {Hoogeboom, Emiel and Cohen, Taco S. and Tomczak, Jakub M.},
 date = {30.01.2020},
 title = {Learning Discrete Distributions by Dequantization},
 url = {http://arxiv.org/pdf/2001.11235v1},
 file = {http://arxiv.org/abs/2001.11235v1},
 file = {https://arxiv.org/pdf/2001.11235v1.pdf}
}


@book{Goodfellow.2016,
 abstract = {{\textquotedbl}Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors{\textquotedbl}--Publisher's description



Introduction -- Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models.},
 author = {Goodfellow, Ian and Courville, Aaron and Bengio, Yoshua},
 year = {2016},
 title = {Deep learning},
 url = {https://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=2565107},
 address = {Cambridge, Massachusetts},
 publisher = {{The MIT Press}},
 isbn = {9780262035613},
 series = {Adaptive computation and machine learning}
}


@misc{Durkan.10.06.2019,
 abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.},
 author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
 date = {10.06.2019},
 year = {2019},
 title = {Neural Spline Flows},
 url = {http://arxiv.org/pdf/1906.04032v2},
 file = {https://arxiv.org/pdf/1906.04032v2.pdf},
 file = {http://arxiv.org/abs/1906.04032v2}
}


@book{Dullemond.2017,
 author = {Dullemond, C. P.},
 year = {2017},
 title = {Radiative Transfer in Astrophysics: Theory, Numerical Methods and Applications - Lecture notes},
 institution = {{University of Heidelberg}}
}


@misc{Dinh.27.05.2016,
 abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
 author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
 date = {27.05.2016},
 year = {2016},
 title = {Density estimation using Real NVP},
 url = {http://arxiv.org/pdf/1605.08803v3},
 file = {http://arxiv.org/abs/1605.08803v3},
 file = {https://arxiv.org/pdf/1605.08803v3.pdf}
}


@article{DiazBaso.2022,
 author = {{D{\'i}az Baso}, C. J. and {Asensio Ramos}, A. and de {La Cruz Rodr{\'i}guez}, J.},
 year = {2022},
 title = {Bayesian Stokes inversion with normalizing flows},
 pages = {A165},
 volume = {659},
 issn = {0004-6361},
 journal = {Astronomy {\&} Astrophysics},
 doi = {10.1051/0004-6361/202142018}
}


@article{delToroIniesta.2016,
 author = {{del Toro Iniesta}, Jose Carlos and {Ruiz Cobo}, Basilio},
 year = {2016},
 title = {Inversion of the radiative transfer equation for polarized light},
 volume = {13},
 number = {1},
 issn = {2367-3648},
 journal = {Living Reviews in Solar Physics},
 doi = {10.1007/s41116-016-0005-2}
}


@book{Deisenroth.2020,
 author = {Deisenroth, Marc Peter and Faisal, A. Aldo and Ong, Cheng Soon},
 year = {2020},
 title = {Mathematics for machine learning},
 address = {Cambridge and New York, NY and Port Melbourne and New Delhi and Singapore},
 publisher = {{Cambridge University Press}},
 isbn = {9781108455145},
 doi = {10.1017/9781108679930},
 file = {https://www.gbv.de/dms/tib-ub-hannover/1679469878.pdf}
}


@book{DeglInnocenti.2005,
 abstract = {Description of Polarized Radiation -- Angular Momentum and Racah Algebra -- Atomic Spectroscopy -- Quantization of the Electromagnetic Field (Non-Relativistic Theory) -- Interaction of Material Systems with Polarized Radiation (the Classical Approach) -- Interaction of Material Systems with Polarized Radiation (the Quantum Approach) -- Statistical Equilibrium Equations and Radiative Transfer Coefficients for Atomic Systems -- Radiative Transfer for Polarized Radiation -- Line Formation in a Magnetic Field -- Non-Equilibrium Atomic Physics -- Astrophysical Applications: Solar Magnetometry -- Astrophysical Applications: Radiation Anisotropy in Stellar Atmospheres -- Astrophysical Applications: the Outer Layers of Stellar Atmospheres -- Astrophysical Applications: Stellar Atmospheres.},
 author = {Degl'Innocenti, Egidio Landi},
 year = {2005},
 title = {Polarization in Spectral Lines},
 address = {Dordrecht},
 volume = {307},
 publisher = {{Springer Netherlands}},
 isbn = {1-4020-2415-0},
 series = {Springer eBook Collection Physics and Astronomy},
 doi = {10.1007/1-4020-2415-0},
 file = {https://swbplus.bsz-bw.de/bsz264337808cov.jpg}
}


@article{BondTaylor.2022,
 abstract = {Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations.},
 author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
 year = {2022},
 title = {Deep Generative Modelling: A Comparative Review of VAEs, GANs,  Normalizing Flows, Energy-Based and Autoregressive Models},
 url = {http://arxiv.org/pdf/2103.04922v4},
 pages = {7327--7347},
 volume = {44},
 number = {11},
 issn = {0162-8828},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 doi = {10.1109/TPAMI.2021.3116668},
 file = {https://arxiv.org/pdf/2103.04922v4.pdf},
 file = {http://arxiv.org/abs/2103.04922v4}
}


@book{Rybicki.2008,
 author = {Rybicki, George B. and Lightman, Alan P.},
 year = {2008},
 title = {Radiative Processes in Astrophysics},
 keywords = {Astrophysik;Strahlungsprozess},
 address = {Weinheim},
 edition = {1., Auflage, neue Ausg},
 publisher = {Wiley-VCH},
 isbn = {352761818X},
 file = {https://d-nb.info/1055831460/34},
 file = {https://nbn-resolving.org/urn:nbn:de:101:1-2014081524485},
 file = {http://d-nb.info/1055831460}
}


@book{Stix.2002,
 abstract = {A wealth of new experimental and theoretical results has been obtained in solar physics since the first edition of this textbook appeared in 1989. Thus all nine chapters have been thoroughly revised, and about 100 pages and many new illustrations have been added to the text. The additions include element diffusion in the solar interior, the recent neutrino experiments, methods of image restoration, observational devices used for spectroscopy and polarimetry, and new developments in helioseismology and numerical simulation. The book takes particular advantage of the results of several recent space missions, which lead to substantial progress in our understanding of the Sun, from the deep interior to the corona and solar wind},
 author = {Stix, Michael},
 year = {2002},
 title = {The Sun: An Introduction},
 address = {Berlin and Heidelberg},
 edition = {Second Edition},
 publisher = {Springer},
 isbn = {978-3-642-62477-3},
 series = {Astronomy and Astrophysics Library},
 doi = {10.1007/978-3-642-56042-2}
}


